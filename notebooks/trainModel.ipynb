{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division  # Python 2 users only\n",
    "\n",
    "import pandas as pd\n",
    "from pygate import *\n",
    "from pygate.prs.ml import *\n",
    "from pygate.prs.rule import *\n",
    "from pygate.utils import TreeUtils\n",
    "import json\n",
    "from nltk.tree import ParentedTree\n",
    "import os\n",
    "from pygate.export.brat import BratServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in claim label dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claim_df = pd.read_excel('./data/CE-ACL-14/2014_7_18_ibm_CDCdata.xls')\n",
    "# evid_df=pd.read_excel('data/CE-ACL-14/2014_7_18_ibm_CDEdata.xls')\n",
    "claim_df['Claim'] = claim_df['Claim'].apply(lambda x: x.encode(\"ascii\", \"ignore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classes defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentLabelAnnotator(PR):    \n",
    "    def __init__(self, labelDataFrame):\n",
    "        self.df=labelDataFrame\n",
    "         \n",
    "    def process(self, doc):\n",
    "        df=self.df\n",
    "        article_id =doc.getDocFeature('file_id');\n",
    "        article_id=article_id.replace('_', ' ')        \n",
    "        df= df[df['Article'] == article_id]\n",
    "#         print \"processing\" , df.shape[0]\n",
    "        for sent in doc.getSents():\n",
    "            sent.setLabel('class',0)\n",
    "            for claim in df['Claim']:\n",
    "                if claim in sent.text:\n",
    "                    sent.setLabel('class',1)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClauseLabelAnnotator(PR):    \n",
    "    def __init__(self, labelDataFrame):\n",
    "        self.df=labelDataFrame\n",
    "         \n",
    "    def process(self, doc):\n",
    "        df=self.df\n",
    "        article_id =doc.getId();\n",
    "        article_id=article_id.replace('_', ' ')        \n",
    "        df= df[df['Article'] == article_id]\n",
    "#         print \"processing\" , df.shape[0]\n",
    "        for clause in doc['SubClause']:\n",
    "            clause.setLabel('class',0)\n",
    "            for claim in df['Claim']:\n",
    "                if claim in clause.text:\n",
    "                    clause.setLabel('class',1) \n",
    "#                     clause.setLabel('claim-id', cla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubClauseAnnotator(PR):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def process(self,doc):\n",
    "        self.sBar=False\n",
    "        self.clauses=[]\n",
    "        sents=doc.getSents()\n",
    "\n",
    "        for s in sents:\n",
    "            parse=s.getFeature('constituency-parse')\n",
    "            tree=ParentedTree.fromstring(parse)\n",
    "            subclauses=[]\n",
    "            self.traverseTree(tree, subclauses )\n",
    "            annots=self.makeAnnotations(s, tree, subclauses)\n",
    "            self.clauses.extend(annots)\n",
    "\n",
    "        doc.setAnnotationSet('SubClause', self.clauses)\n",
    "\n",
    "    def traverseTree(self, tree, clauses, sBar=False):\n",
    "        label=tree.label()\n",
    "#         print(\"node:\", label )\n",
    "        if(label=='SBAR'):\n",
    "            sBar=True\n",
    "        if(sBar and label=='S'):\n",
    "            clauses.append(tree)\n",
    "            sBar=False\n",
    "        for subtree in tree:\n",
    "            if type(subtree) == ParentedTree:\n",
    "                self.traverseTree(subtree,clauses,sBar)\n",
    "\n",
    "    def makeAnnotations(self, sent, tree, subclauses):\n",
    "        annots=[]\n",
    "        doc=sent.getDoc()\n",
    "        if len(subclauses)>0:\n",
    "#             tree.draw()\n",
    "            for clause in subclauses:\n",
    "                tokens=TreeUtils.getTokensWithinSubtree(sent, tree, clause )\n",
    "                tStart=tokens[0].tStart\n",
    "                tEnd=tokens[-1].tEnd+1\n",
    "                cStart=tokens[0].cStart\n",
    "                cEnd=tokens[-1].cEnd                \n",
    "                text=doc.getText()[cStart:cEnd]\n",
    "                ann=Annotation(text,tStart,tEnd,cStart,cEnd,'SubClause')\n",
    "                ann.setFeature('sent-index', sent.getFeature('index'))\n",
    "                ann.setRelation('tokens', tokens)\n",
    "                annots.append(ann)\n",
    "            for ann in annots:\n",
    "                ann.setFeature('full-sent', False)\n",
    "                ann.setFeature('num-clauses', len(annots))\n",
    "        else:\n",
    "            ann=Annotation(sent.text, sent.tStart, sent.tEnd, sent.cStart, sent.cEnd, 'SubClause')\n",
    "            ann.setFeature('sent-index', sent.getFeature('index'))\n",
    "            ann.setFeature('full-sent', True)\n",
    "            ann.setRelation('tokens', sent.getRelation('tokens'))\n",
    "            annots.append(ann)           \n",
    "        sent.setRelation('subClauses', annots)\n",
    "        return annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ds2=DocumentStore('./docstore/train_docs')\n",
    "docStore=DocumentStore('./docstore/train_docs')\n",
    "docStore.loadDocs(fileids='.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do Subclause annotations in 1 pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe_1=Pipeline(docStore)\n",
    "prs_1=[\n",
    "    SentLabelAnnotator(claim_df), # optional, we dont predict at sentence level\n",
    "    SubClauseAnnotator(),\n",
    "    ClauseLabelAnnotator(claim_df)\n",
    "]\n",
    "pipe_1.setPRs(prs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results=pipe_1.process()\n",
    "if len(results[1])>0: #if there were exceptions\n",
    "    ex=results[1][1]\n",
    "    ex.printStackTrace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleDoc=docStore.docMap()['Criticism_of_atheism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbls=[a.getLabel('class') for a in sampleDoc['SubClause']]\n",
    "print np.sum(lbls)\n",
    "for sc in sampleDoc['SubClause']:\n",
    "    if sc.getLabel('class')==1:\n",
    "        print '_______________________'\n",
    "        print sc\n",
    "        print sc.getFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections as coll\n",
    "\n",
    "class FeatureExtractorPR(PR):\n",
    "    def process(self, doc):\n",
    "        for span in doc['SubClause']:\n",
    "            tokens=span.getRelation('tokens')\n",
    "            self.extractTokenFeatures(tokens, span)\n",
    "   \n",
    "    def extractTokenFeatures(self, tokens, span):          \n",
    "        tknFeats=[t.getFeatures() for t in tokens] \n",
    "        ngramFeatures=self.extractNgrams(['pos'], 2, tokens)  \n",
    "        posLemFeatures=self.lemmatizePos(['VBZ', 'RB'], tokens)\n",
    "        span.updateFeatures(ngramFeatures)\n",
    "        span.updateFeatures(posLemFeatures)\n",
    "                   \n",
    "    def extractNgrams(self, fnames, n, tokens):\n",
    "        START='<S>'\n",
    "        END='<E>'\n",
    "        f_b={ key: START for key in fnames}       \n",
    "        f_e={ key: END for key in fnames}\n",
    "        ngrams=coll.Counter()\n",
    "        feats=[tkn.getFeatures() for tkn in tokens]\n",
    "        for i in range(n-1):\n",
    "            feats.insert(0,f_b)\n",
    "            feats.append(f_e)\n",
    "            \n",
    "        for i in range(len(feats)-n+1):\n",
    "            merge=feats[i:i+n]\n",
    "            for name in fnames:\n",
    "                mergedFeat=\"\".join( [name, str(n),'/' ]+[f[name]+\"-\" for f in merge])[0:-1]\n",
    "                ngrams[mergedFeat]+=1            \n",
    "        return ngrams \n",
    "    \n",
    "    def lemmatizePos(self, pos_types, tokens):\n",
    "        posFeats=coll.Counter()\n",
    "        for t in tokens:\n",
    "            feat=t.getFeatures()\n",
    "            tkn_pos=feat['pos']\n",
    "            if tkn_pos in pos_types:\n",
    "                posFeats[\"lempos/\"+tkn_pos+\"-\"+feat['lemma']]+=1\n",
    "        return posFeats\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fe=FeatureExtractorPR()\n",
    "fe.process(sampleDoc)\n",
    "sampleDoc['SubClause'][0].getFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sample test\n",
    "from nltk import MaxentClassifier\n",
    "\n",
    "# model=MaxentClassifier.train([({},1)] )\n",
    "scpr=ClassifierPR(MaxentClassifier, 'SubClause')\n",
    "# scpr.process(sampleDoc)\n",
    "# scpr.split()\n",
    "# scpr.balanceDataset([(1,1),(0,1.5)])\n",
    "\n",
    "# scpr.train(modelParams={'max_iter':1})\n",
    "# scpr.save('sample.pr')\n",
    "# cpr.train(modelParams={'max_iter':10})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Pipeline for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPipe=Pipeline(corpus=docStore)\n",
    "cpr=ClassifierPR(MaxentClassifier, 'SubClause')\n",
    "prs=[FeatureExtractorPR(),\n",
    "     cpr]\n",
    "trainPipe.setPRs(prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res=trainPipe.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpr.split()\n",
    "cpr.balanceDataset([(1,1),(0,2)])\n",
    "cpr.train(modelParams={'max_iter':100})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpr.save('./maxent.pr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sasinda'\n",
    "from pygate import PR\n",
    "from nltk import classify\n",
    "import sklearn.metrics\n",
    "import collections as coll\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import pickle\n",
    "\n",
    "\n",
    "class ClassifierPR(PR):\n",
    "    '''\n",
    "    @param model= needs to be a NLTK ML model. Wrap scikit models as NLTK's SklearnClassifier.\n",
    "    @param outputKey = the label name that the precicted annotation will have.\n",
    "    @param mode = inference: for running model against test samples\n",
    "                  train: for collecting the training dataset.\n",
    "                  Need to explicitly run train() after the pipeline is complete to start training the model.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, modelClass, level, featureFilter=None, inputLabel='class', outputLabel='pred', mode='train'):\n",
    "        self.modelClass = modelClass\n",
    "        self.model = None\n",
    "        self.mode = mode\n",
    "        self.level = level\n",
    "        self.outputLabel = outputLabel\n",
    "        self.inputLabel = inputLabel\n",
    "        self.filtr = featureFilter\n",
    "        self.data_set = {'all': []}\n",
    "\n",
    "    def process(self, doc):\n",
    "        level = self.level\n",
    "        outputKey = self.outputLabel\n",
    "        if self.mode == 'train':\n",
    "            self.collectTrainSet(doc)\n",
    "        elif self.mode == 'inference':\n",
    "            self.predict(doc)\n",
    "\n",
    "    def collectTrainSet(self, doc):\n",
    "        if self.filtr:\n",
    "            feats = self.__filterFeatures(self.filtr)\n",
    "        else:\n",
    "            feats = [(t.getFeatures(), t.getLabel(self.inputLabel)) for t in doc[self.level]]\n",
    "        self.data_set['all'].extend(feats)\n",
    "\n",
    "    def __filterFeatures(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def getDataSet(self, name):\n",
    "        ''' trainSet is [(features:{}, label:str)] list of tuples'''\n",
    "        return self.data_set[name]\n",
    "\n",
    "    def split(self, splits={'validation': 0.2, 'train': 0.8}):\n",
    "        '''splits the dataset into train and validation'''\n",
    "        all_data = self.data_set['all']\n",
    "        start = 0\n",
    "        for key, value in splits.iteritems():\n",
    "            split = int(len(all_data) * value)\n",
    "            self.data_set[key] = all_data[start:start + split]\n",
    "            start += split\n",
    "\n",
    "    def train(self, modelParams={}, dataset_name='train', validate=True):\n",
    "        '''\n",
    "#         @param classRatios: list of tuples, class label and ratio.\n",
    "          @param positive to negative ratio as a tuple.\n",
    "        '''\n",
    "\n",
    "        if not self.data_set.has_key(dataset_name): self.split()\n",
    "        if not self.data_set.has_key(dataset_name): raise ValueError(\n",
    "            \"Please split the dataset before training. Call to this method ran split with default parameters, so you may use dataset_name as train, or explicitly split\")\n",
    "\n",
    "        train_set = self.data_set['train']\n",
    "        self.model = self.modelClass.train(train_set, **modelParams)\n",
    "\n",
    "        if validate: self.validate()\n",
    "\n",
    "    def validate(self, dataset_name='validation'):\n",
    "\n",
    "        if not self.data_set.has_key(dataset_name):  raise ValueError(\"Please split the dataset before validation\")\n",
    "\n",
    "        val_set = self.data_set[dataset_name]\n",
    "        predlist = self.model.classify_many([feat for feat, label in val_set])\n",
    "        truelist = []\n",
    "        for i in range(len(predlist)):\n",
    "            actual_label = val_set[i][1]\n",
    "            truelist.append(actual_label)\n",
    "\n",
    "        print 'pos precision:', sklearn.metrics.precision_score(truelist, predlist, pos_label=1)\n",
    "        print 'pos recall:', sklearn.metrics.recall_score(truelist, predlist, pos_label=1)\n",
    "        print 'pos F-measure:', sklearn.metrics.f1_score(truelist, predlist, pos_label=1)\n",
    "        print 'neg precision:', sklearn.metrics.precision_score(truelist, predlist, pos_label=0)\n",
    "        print 'neg recall:', sklearn.metrics.recall_score(truelist, predlist, pos_label=0)\n",
    "        print 'neg F-measure:', sklearn.metrics.f1_score(truelist, predlist, pos_label=0)\n",
    "\n",
    "    def trainCrossValidate(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def balanceDataset(self, classRatios, data_set_from='train', data_set_to='train', randomShuffle=True):\n",
    "        balancer = coll.defaultdict(list)\n",
    "        for datum in self.data_set[data_set_from]:\n",
    "            cls = datum[1]\n",
    "            balancer[cls].append(datum)\n",
    "\n",
    "        common_denom = len(self.data_set[data_set_from])\n",
    "        for cls, ratio_term in classRatios:\n",
    "            common_denom = min(common_denom, len(balancer[cls]) / ratio_term)\n",
    "\n",
    "        balancedSet = []\n",
    "        for cls, ratio_term in classRatios:\n",
    "            balancedSet.extend(balancer[cls][:int(common_denom * ratio_term)])\n",
    "\n",
    "        if randomShuffle:\n",
    "            np.random.shuffle(balancedSet)\n",
    "        self.data_set[data_set_to] = balancedSet\n",
    "\n",
    "    def predict(self, doc):\n",
    "        feats = [t.getFeatures() for t in doc[self.level]]\n",
    "        preds = self.model.classify_many(feats)\n",
    "        for i, ann in enumerate(doc[self.level]):\n",
    "            ann.setLabel(self.outputLabel, preds[i])\n",
    "\n",
    "    def save(self, prFile, setMode='inference'):\n",
    "        me = self\n",
    "        me.mode = setMode\n",
    "        if setMode == 'inference':\n",
    "            me = copy(self)\n",
    "            me.data_set = None\n",
    "            me.balanced_data_set = None\n",
    "\n",
    "        super(self.__class__, me).save(prFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pygate.ext.stanford import StanfordAnnotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cprProd=ClassifierPR.load('./maxent.pr')\n",
    "def printPreds(doc):\n",
    "    for sc in doc['SubClause']:\n",
    "        if sc.getLabel('pred')==1:\n",
    "            print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "claimAnn=SPMRulePR('@SubClause.labels.pred==1 -->  @Claim')\n",
    "entityAnn= SPMRulePR(\"(@Token.features.ner!='O')+:span --> @Entity:e ->  e.subType=span.features.ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe=Pipeline()\n",
    "prs=[StanfordAnnotator(cacheDir='./inf_corenlp'),\n",
    "     SubClauseAnnotator(),\n",
    "     FeatureExtractorPR(),     \n",
    "     cprProd,\n",
    "     claimAnn,\n",
    "     entityAnn\n",
    "    ]\n",
    "pipe.setPRs(prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc([Jimmy Carter, Seeing Resurgence of Racism, Plans Baptist Conference for Unity.//Sentence:0//tidx0:14]//sentf[{'dep-parse': 'not implemented!', 'index': 0, 'constituency-parse': u'(ROOT\\n  (S\\n    (NP\\n      (//tknf[{'lemma': u'Jimmy', 'ner': u'PERSON', 'pos': u'NNP', 'index': 1}])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news1=Document(\n",
    "u\"\"\"Jimmy Carter, Seeing Resurgence of Racism, Plans Baptist Conference for Unity.\n",
    "\n",
    "Former President Jimmy Carter, who has long put religion and racial reconciliation at the center of his life, is on a mission to heal a racial divide among Baptists and help the country soothe rifts that he believes are getting worse.\n",
    "\n",
    "In an interview on Monday, Mr. Carter spoke of a resurgence of open racism, saying, “I don’t feel good, except for one thing: I think the country has been reawakened the last two or three years to the fact that we haven’t resolved the race issue adequately.”\n",
    "\n",
    "He said that Republican animosity toward President Obama had “a heavy racial overtone” and that Donald J. Trump’s surprisingly successful campaign for president had “tapped a waiting reservoir there of inherent racism.”\n",
    "\n",
    "Mr. Carter conducted telephone interviews to call attention to a summit meeting he plans to hold in Atlanta this fall to bring together white, black, Hispanic and Asian Baptists to work on issues of race and social inequality. Mr. Carter began the effort, called the New Baptist Covenant, in 2007, but it has taken root in only a few cities. The initiative is expanding to enlist Baptist congregations across the country to unite across racial lines.\n",
    "\n",
    "Mr. Carter, 91, began treatment last year for cancer that had started in his liver and spread to his brain. He announced in December that doctors had found him free of cancer but that he was still receiving treatments for metastatic melanoma. On Monday, he said he was feeling well.\n",
    "\n",
    "Mr. Carter, a Democrat who was the 39th president, grew up on a farm in Plains, Ga., where many of his friends were the black children of neighboring farmhands. He was raised a Southern Baptist and was the first United States president to call himself a born-again Christian, bringing national attention to the evangelical movement.\n",
    "\n",
    "Mr. Carter said the election of Mr. Obama was a hopeful sign, but he added, “I think there’s a heavy reaction among some of the racially conscious Republicans against an African-American being president.”\n",
    "\n",
    "He said recent reports showing high unemployment and incarceration rates among black people, “combined with the white police attacks on innocent blacks,” had “reawakened” the country to the realization that racism was not resolved in the 1960s and ’70s.\n",
    "\n",
    "He said Mr. Trump had violated “basic human rights” when he referred to Mexican immigrants as criminals and called for a ban on Muslims’ entering the country.\n",
    "\n",
    "“When you single out any particular group of people for secondary citizenship status, that’s a violation of basic human rights,” said Mr. Carter, who won the Nobel Peace Prize in 2002 for his work with the Carter Center in promoting human rights and democracy in many countries.\n",
    "\n",
    "Asked why polls showed high support among evangelical Christians for Mr. Trump’s candidacy, Mr. Carter said: “The use of the word evangelical is a misnomer. I consider myself an evangelical as well. And obviously, what most of the news reporters thought were evangelicals are conservative Republicans.”\n",
    "\n",
    "“They have a heavy orientation to right-wing political philosophy, and he obviously is a proponent of that concept,” Mr. Carter said, referring to Mr. Trump.\n",
    "\n",
    "He pointed out that the evangelicals in the Southern Baptist Convention had aligned themselves with the Republican Party and organized the Moral Majority, a conservative Christian political group, only in the late 1970s, while he was president. Mr. Carter announced that he was leaving the Southern Baptist Convention in 2000, after the denomination solidified its turn to the right and declared that it would not accept women as pastors.\n",
    "\n",
    "Mr. Carter founded the New Baptist Covenant by reaching out to black and white Baptist associations, many of which had split many years ago over slavery. Nearly 15,000 people from 30 Baptist associations attended the founding meeting in 2008.\n",
    "\n",
    "Hannah McMahan, the executive director of the New Baptist Covenant, said the group had been in a “pilot phase” for the last two years. She said black and white churches had formed partnerships, called covenants, in Dallas; Macon, Ga.; St. Louis; Birmingham, Ala.; and Atlanta.\n",
    "\n",
    "But the process is painstaking, Ms. McMahan said, adding, “What this has given me an appreciation for is how deep the divides are, and that this kind of work will not happen overnight.”\n",
    "\n",
    "The work is especially challenging in this climate, said the Rev. Raphael G. Warnock, the senior pastor of Ebenezer Baptist Church in Atlanta, the church where the Rev. Martin Luther King Jr. was once a pastor. Ebenezer Baptist is participating in the New Baptist Covenant.\n",
    "\n",
    "“This is a dark moment in our national conversation,” Pastor Warnock said. “Those of us who understand that we are better together had better raise our voices, because there are others who are trafficking in theater, in paranoia, and they ply the trade of fear as part of their political craft.”\n",
    "\n",
    "However, he said, “I’m much more fired up than discouraged, because the ugliness of the rhetoric we’re seeing in this election cycle really just brings into sharp focus the ugly underbelly of bigotry that has always been there.”\n",
    "\"\"\"               )\n",
    "news1.setId('sample1')\n",
    "pipe.processDoc(news1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mr. Carter conducted telephone interviews to call attention to a summit meeting he plans to hold in Atlanta this fall to bring together white, black, Hispanic and Asian Baptists to work on issues of race and social inequality.//SubClause:9//tidx153:194\n",
      "there’s a heavy reaction among some of the racially conscious Republicans against an African-American being president//SubClause:20//tidx375:392\n",
      "this has given me an appreciation for is how deep the divides are, and that this kind of work will not happen overnight//SubClause:40//tidx801:825\n",
      "there are others who are trafficking in theater, in paranoia//SubClause:50//tidx911:922\n",
      "are trafficking in theater, in paranoia//SubClause:51//tidx915:922\n"
     ]
    }
   ],
   "source": [
    "printPreds(news1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"700\"\n",
       "            src=\"http://localhost:8001/index.xhtml#/povmapsample1\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10e4f2e90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=BratServer(dataDir='~/CMProject/viz/brat-v1.3_Crunchy_Frog/data/povmap')\n",
    "bs.draw(news1, ['Entity', 'Claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Jimmy//Token:0//tidx0:0,\n",
       " Carter//Token:1//tidx1:1,\n",
       " Plans//Token:8//tidx8:8,\n",
       " Baptist//Token:9//tidx9:9,\n",
       " Conference//Token:10//tidx10:10,\n",
       " for//Token:11//tidx11:11,\n",
       " Unity//Token:12//tidx12:12,\n",
       " Jimmy//Token:16//tidx16:16,\n",
       " Carter//Token:17//tidx17:17,\n",
       " Baptists//Token:44//tidx44:44,\n",
       " Monday//Token:62//tidx62:62,\n",
       " Carter//Token:65//tidx65:65,\n",
       " one//Token:85//tidx85:85,\n",
       " last//Token:96//tidx96:96,\n",
       " two//Token:97//tidx97:97,\n",
       " or//Token:98//tidx98:98,\n",
       " three//Token:99//tidx99:99,\n",
       " years//Token:100//tidx100:100,\n",
       " Republican//Token:118//tidx118:118,\n",
       " Obama//Token:122//tidx122:122,\n",
       " Donald//Token:132//tidx132:132,\n",
       " J.//Token:133//tidx133:133,\n",
       " Trump//Token:134//tidx134:134,\n",
       " Carter//Token:154//tidx154:154,\n",
       " Atlanta//Token:170//tidx170:170,\n",
       " this//Token:171//tidx171:171,\n",
       " fall//Token:172//tidx172:172,\n",
       " Asian//Token:182//tidx182:182,\n",
       " Baptists//Token:183//tidx183:183,\n",
       " Carter//Token:195//tidx195:195,\n",
       " 2007//Token:207//tidx207:207,\n",
       " Baptist//Token:226//tidx226:226,\n",
       " Carter//Token:238//tidx238:238,\n",
       " 91//Token:240//tidx240:240,\n",
       " last//Token:244//tidx244:244,\n",
       " year//Token:245//tidx245:245,\n",
       " December//Token:263//tidx263:263,\n",
       " Monday//Token:284//tidx284:284,\n",
       " Carter//Token:294//tidx294:294,\n",
       " Democrat//Token:297//tidx297:297,\n",
       " 39th//Token:301//tidx301:301,\n",
       " Plains//Token:310//tidx310:310,\n",
       " Ga.//Token:312//tidx312:312,\n",
       " Southern//Token:331//tidx331:331,\n",
       " Baptist//Token:332//tidx332:332,\n",
       " first//Token:336//tidx336:336,\n",
       " United//Token:337//tidx337:337,\n",
       " States//Token:338//tidx338:338,\n",
       " Christian//Token:345//tidx345:345,\n",
       " Carter//Token:356//tidx356:356,\n",
       " Obama//Token:362//tidx362:362,\n",
       " Republicans//Token:386//tidx386:386,\n",
       " African-American//Token:389//tidx389:389,\n",
       " the//Token:435//tidx435:435,\n",
       " 1960s//Token:436//tidx436:436,\n",
       " ’70s//Token:438//tidx438:438,\n",
       " Trump//Token:443//tidx443:443,\n",
       " Mexican//Token:455//tidx455:455,\n",
       " Muslims//Token:465//tidx465:465,\n",
       " Carter//Token:498//tidx498:498,\n",
       " Nobel//Token:503//tidx503:503,\n",
       " Peace//Token:504//tidx504:504,\n",
       " Prize//Token:505//tidx505:505,\n",
       " 2002//Token:507//tidx507:507,\n",
       " Carter//Token:513//tidx513:513,\n",
       " Center//Token:514//tidx514:514,\n",
       " Christians//Token:533//tidx533:533,\n",
       " Trump//Token:536//tidx536:536,\n",
       " Carter//Token:541//tidx541:541,\n",
       " Republicans//Token:577//tidx577:577,\n",
       " Carter//Token:603//tidx603:603,\n",
       " Trump//Token:609//tidx609:609,\n",
       " Southern//Token:619//tidx619:619,\n",
       " Baptist//Token:620//tidx620:620,\n",
       " Convention//Token:621//tidx621:621,\n",
       " Republican//Token:627//tidx627:627,\n",
       " Party//Token:628//tidx628:628,\n",
       " Christian//Token:637//tidx637:637,\n",
       " 1970s//Token:645//tidx645:645,\n",
       " Carter//Token:653//tidx653:653,\n",
       " 2000//Token:664//tidx664:664,\n",
       " Carter//Token:687//tidx687:687,\n",
       " Baptist//Token:700//tidx700:700,\n",
       " years//Token:709//tidx709:709,\n",
       " ago//Token:710//tidx710:710,\n",
       " 15,000//Token:715//tidx715:715,\n",
       " 30//Token:718//tidx718:718,\n",
       " Baptist//Token:719//tidx719:719,\n",
       " 2008//Token:726//tidx726:726,\n",
       " Hannah//Token:728//tidx728:728,\n",
       " McMahan//Token:729//tidx729:729,\n",
       " New//Token:736//tidx736:736,\n",
       " Baptist//Token:737//tidx737:737,\n",
       " Covenant//Token:738//tidx738:738,\n",
       " the//Token:752//tidx752:752,\n",
       " last//Token:753//tidx753:753,\n",
       " two//Token:754//tidx754:754,\n",
       " years//Token:755//tidx755:755,\n",
       " Dallas//Token:771//tidx771:771,\n",
       " Macon//Token:773//tidx773:773,\n",
       " Ga.//Token:775//tidx775:775,\n",
       " St.//Token:777//tidx777:777,\n",
       " Louis//Token:778//tidx778:778,\n",
       " Birmingham//Token:780//tidx780:780,\n",
       " Ala.//Token:782//tidx782:782,\n",
       " Atlanta//Token:785//tidx785:785,\n",
       " McMahan//Token:794//tidx794:794,\n",
       " overnight//Token:824//tidx824:824,\n",
       " Raphael//Token:839//tidx839:839,\n",
       " G.//Token:840//tidx840:840,\n",
       " Warnock//Token:841//tidx841:841,\n",
       " Ebenezer//Token:847//tidx847:847,\n",
       " Baptist//Token:848//tidx848:848,\n",
       " Church//Token:849//tidx849:849,\n",
       " Atlanta//Token:851//tidx851:851,\n",
       " Martin//Token:858//tidx858:858,\n",
       " Luther//Token:859//tidx859:859,\n",
       " King//Token:860//tidx860:860,\n",
       " Jr.//Token:861//tidx861:861,\n",
       " once//Token:863//tidx863:863,\n",
       " Ebenezer//Token:867//tidx867:867,\n",
       " Baptist//Token:868//tidx868:868,\n",
       " Warnock//Token:890//tidx890:890]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': 1}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
